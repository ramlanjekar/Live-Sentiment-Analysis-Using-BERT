# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Using_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qWyVisOntO_pOXzSBNiE9W736wLkPcbY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

!pip install transformers
!pip install torchmetrics

"""##Load up the libraries"""

from google.colab import drive
drive.mount('/content/drive')

import torch

# Check if CUDA is available and set the device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import pandas as pd
import numpy as np
import re
from transformers import AutoTokenizer, BertModel, BertForSequenceClassification, BertConfig
from tqdm import tqdm
import torch
import pickle
from torch.utils.data import TensorDataset, DataLoader
from torchmetrics import Accuracy

"""## A function to pre-process each line"""

def preprocess(x):
    x = re.sub('<.*?>', ' ', x)
    x = re.sub('http\S+', ' ', x)
    x = re.sub('\s+', ' ', x)
    return x.lower().strip()

"""## Helper functions to save and load pickle files"""

# These functions help in persisting and retrieving Python objects to and from files,
# making it easy to save the state of a program and reload it later.

import pickle

def save_pickle_file(object, file_name):
    with open(file_name, "wb") as fp:
        pickle.dump(object, fp)

def load_pickle_file(file_name):
    with open(file_name, "rb") as fp:
        data = pickle.load(fp)
    return data

"""## This function converts input dataframe to transformer usable format"""

def pipeline(dataframe):
    # Pre-process the sentences
    dataframe['review'] = dataframe['review'].apply(lambda x: preprocess(x))

    # Pre-pend CLS token to each sentence
    sentences = ["[CLS] " + s for s in dataframe['review'].values]

    # Extract labels
    labels = dataframe['sentiment'].values

    # Tokenize each sentence
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)
    tokenized = [tokenizer.tokenize(s) for s in tqdm(sentences)]

    # Append the SEP token and also set a threshold for the number of tokens in a sentence
    MAX_LEN_TRAIN, MAX_LEN_TEST = 500, 500
    tokenized = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized]

    # Generate IDs of each token and add padding to sentences smaller than given threshold
    ids = [tokenizer.convert_tokens_to_ids(t) for t in tqdm(tokenized)]
    ids = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), mode='constant') for i in ids])

    # Also generate Attention masks. An attention mask is a binary tensor
    # that indicates the position of padded indices so that the model does not attend to them
    # float (1.0 if true, 0.0 if false)
    amasks = np.asarray([[float(i>0) for i in seq] for seq in tqdm(ids)])

    return torch.tensor(ids), torch.tensor(labels), torch.tensor(amasks)

    # The final hidden state of the [CLS] token (after passing through all the transformer layers in BERT) serves as a summary or aggregate representation of the whole input sequence.
# This summary representation is particularly useful for classification tasks because it captures information from the entire sequence in a single vector

"""## Load the training and validation datasets"""

df = pd.read_csv("/content/drive/MyDrive/IMDB Dataset.csv")
df.head()

df['sentiment'] = df['sentiment'].map({'positive':1, 'negative': 0})

value_counts = df['sentiment'].value_counts()
print(value_counts)

df.head()

df['sentiment'].value_counts()

from sklearn.model_selection import train_test_split

# Perform stratified split
train_reviews, val_reviews, train_sentiments, val_sentiments = train_test_split(
    df['review'],
    df['sentiment'],
    test_size=0.2,
    stratify=df['sentiment'],
    random_state=42
)

# Convert splits back to DataFrame
df_train = pd.DataFrame({'review': train_reviews, 'sentiment': train_sentiments})
df_val = pd.DataFrame({'review': val_reviews, 'sentiment': val_sentiments})

"""## Clean the data and store in BERT usable format"""

ids_train, labels_train, amasks_train = pipeline(df_train)
ids_val, labels_val, amasks_val = pipeline(df_val)

print(ids_train.shape, amasks_train.shape, labels_train.shape)
print(ids_val.shape, amasks_val.shape, labels_val.shape)

"""## Generate the data loaders"""

train_set = TensorDataset(ids_train, amasks_train, labels_train)
train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)

val_set = TensorDataset(ids_val, amasks_val, labels_val)
val_dataloader = DataLoader(val_set, batch_size=32, shuffle=False)

"""## Now Create the model"""

import torch
from transformers import BertForSequenceClassification

# Load the pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Print the model architecture
print(model)

model =model.to(device)

# Freeze all layers except the classifier and the last two encoder layers
for name, param in model.named_parameters():
    if "classifier" in name or "bert.encoder.layer.10" in name or "bert.encoder.layer.11" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

# Count and print the number of trainable parameters
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Total trainable parameters:", total_params)

"""## Train and Test the model"""

epochs = 5
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, eps=1e-8)
criterion = torch.nn.CrossEntropyLoss()
train_acc = Accuracy(task="binary", num_classes=2).to(device)  # Move to device
val_acc = Accuracy(task="binary", num_classes=2).to(device)    # Move to device

# Training loop
for epoch in range(epochs):
    train_loss, val_loss = list(), list()
    print("\n\nEpoch:", epoch + 1, "\n-----------------------\n")

    # Training phase
    model.train()
    for idx, (x_ids, x_masks, x_labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
        # Clear previous gradients
        optimizer.zero_grad()
        # Move the batch to the model's device
        x_ids, x_masks, x_labels = x_ids.to(device), x_masks.to(device), x_labels.to(device)
        # Perform predictions
        preds = model(x_ids, attention_mask=x_masks)
        # Update train accuracy
        train_acc.update(torch.argmax(preds.logits, dim=1), x_labels)
        # Calculate loss
        loss = criterion(preds.logits, x_labels)
        train_loss.append(loss.item())
        # Calculate gradients
        loss.backward()
        # Update parameters
        optimizer.step()

    # Validation phase
    model.eval()
    for idx, (x_ids, x_masks, x_labels) in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):
        x_ids, x_masks, x_labels = x_ids.to(device), x_masks.to(device), x_labels.to(device)
        preds = model(x_ids, attention_mask=x_masks)
        loss = criterion(preds.logits, x_labels)
        val_loss.append(loss.item())
        val_acc.update(torch.argmax(preds.logits, dim=1), x_labels)

    # Print average losses and accuracies
    print("Train Loss =", sum(train_loss)/len(train_loss), "\tVal Loss =", sum(val_loss)/len(val_loss))
    print("Train Acc =", train_acc.compute().item(), "\tVal Acc =", val_acc.compute().item())

    # Reset metrics for next epoch
    train_acc.reset()
    val_acc.reset()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Assuming you have predictions and true labels prepared
predictions = []
true_labels = []

# Set model to evaluation mode
model.eval()

# Iterate over the validation data loader to get predictions
for idx, (x_ids, x_masks, x_labels) in enumerate(val_dataloader):
    # Move batch to device
    x_ids, x_masks, x_labels = x_ids.to(device), x_masks.to(device), x_labels.to(device)
    # Generate predictions
    with torch.no_grad():
        preds = model(x_ids, attention_mask=x_masks)
    # Convert logits to predictions
    preds = torch.argmax(preds.logits, dim=1)
    predictions.extend(preds.cpu().numpy())
    true_labels.extend(x_labels.cpu().numpy())

# Convert lists to numpy arrays
predictions = np.array(predictions)
true_labels = np.array(true_labels)

# Compute confusion matrix for model's predictions
cm_model = confusion_matrix(true_labels, predictions)

# Create confusion matrix with 100% accuracy for comparison
cm_perfect = np.array([[len(true_labels[true_labels == 0]), 0], [0, len(true_labels[true_labels == 1])]])

# Plot confusion matrices as heatmaps side by side
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(cm_model, annot=True, cmap='Blues', fmt='d', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Model Predictions')

plt.subplot(1, 2, 2)
sns.heatmap(cm_perfect, annot=True, cmap='Blues', fmt='d', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Perfect Predictions (100% Accuracy)')

plt.tight_layout()
plt.show()

import requests
import pandas as pd

# Define your RapidAPI key
rapidapi_key = "6a47f73602msh075c7efd9feed27p121328jsn89286944cf6b"

# Function to fetch IMDb ID by movie title
def fetch_imdb_id(movie_title):
    url = "https://imdb8.p.rapidapi.com/title/find"
    headers = {
        "X-RapidAPI-Key": rapidapi_key,
        "X-RapidAPI-Host": "imdb8.p.rapidapi.com"
    }
    querystring = {"q": movie_title}
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    if "results" in data and len(data["results"]) > 0:
        return data["results"][0]["id"]
    else:
        return None

# Function to fetch IMDb comments with pagination
def fetch_imdb_comments(movie_id, num_comments):
    url = "https://imdb8.p.rapidapi.com/title/get-user-reviews"
    headers = {
        "X-RapidAPI-Key": rapidapi_key,
        "X-RapidAPI-Host": "imdb8.p.rapidapi.com"
    }

    querystring = {"tconst": movie_id.split('/')[-2]}
    comments = []
    pagination_key = None

    while len(comments) < num_comments:
        if pagination_key:
            querystring["paginationKey"] = pagination_key

        response = requests.get(url, headers=headers, params=querystring)
        data = response.json()

        # Extract comments from current page
        current_comments = data.get("reviews", [])
        comments.extend(current_comments)

        # Check if there's a next page
        pagination_key = data.get("paginationKey")

        # Break the loop if no more reviews
        if not pagination_key:
            break

    return comments[:num_comments]

# Function to create DataFrame from comments
def create_dataframe(movie_title, num_comments):
    movie_id = fetch_imdb_id(movie_title)
    if not movie_id:
        print(f"No IMDb ID found for movie title '{movie_title}'")
        return pd.DataFrame()

    comments = fetch_imdb_comments(movie_id, num_comments)
    reviews = []

    for comment in comments:
        review_text = comment.get("reviewText", "")
        reviews.append({"movie_name": movie_title, "reviews": review_text})

    df_train = pd.DataFrame(reviews)
    return df_train

# Function for user input to fetch comments for multiple movies
def fetch_movie_comments():
    movie_titles = []

    while True:
        movie_title = input("Enter the movie title (or press Enter to finish): ")
        if not movie_title:
            break
        movie_titles.append(movie_title)

    num_comments_to_fetch = int(input("Enter the number of comments to fetch for each movie: "))
    print("Fetching comments...")

    all_reviews = []

    for title in movie_titles:
        df_movie = create_dataframe(title, num_comments_to_fetch)
        all_reviews.append(df_movie)

    df_test = pd.concat(all_reviews, ignore_index=True)
    return df_test

# Example usage
df_test = fetch_movie_comments()
print(df_test)

df_train.columns

import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset


def pipeline(dataframe):
    # Pre-process the sentences
    dataframe['reviews'] = dataframe['reviews'].apply(lambda x: preprocess(x))

    # Pre-pend CLS token to each sentence
    sentences = ["[CLS] " + s for s in dataframe['reviews'].values]

    # Tokenize each sentence
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)
    tokenized = [tokenizer.tokenize(s) for s in tqdm(sentences)]

    # Append the SEP token and also set a threshold for the number of tokens in a sentence
    MAX_LEN_TRAIN, MAX_LEN_TEST = 500, 500
    tokenized = [t[:(MAX_LEN_TRAIN-1)] + ['[SEP]'] for t in tokenized]

    # Generate IDs of each token and add padding to sentences smaller than given threshold
    ids = [tokenizer.convert_tokens_to_ids(t) for t in tqdm(tokenized)]
    ids = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), mode='constant') for i in ids])

    amasks = np.asarray([[float(i > 0) for i in seq] for seq in tqdm(ids)])


    return torch.tensor(ids), torch.tensor(amasks)

# Assuming df_train is defined and contains the reviews
ids_train, amasks_train = pipeline(df_test)

train_set = TensorDataset(ids_train, amasks_train)
train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)

model.eval()

# Function to evaluate the model and predict sentiments
def evaluate_model(dataloader):
    predictions = []

    for batch in tqdm(dataloader):
        ids, amasks = batch
        ids = ids.to(device)
        amasks = amasks.to(device)
        with torch.no_grad():
            outputs = model(input_ids=ids, attention_mask=amasks)
            logits = outputs.logits
            predictions_batch = torch.argmax(logits, dim=1)
            predictions.extend(predictions_batch.cpu().numpy())

    return predictions

# Evaluate the model on the test set
predictions = evaluate_model(train_dataloader)

# Add predictions to df_test
df_test['prediction'] = predictions

# Calculate percentage of positive reviews per movie title
results = df_test.groupby('movie_name')['prediction'].apply(lambda x: (x == 1).mean() * 100).reset_index()
results.columns = ['Movie Title', 'Percent Positive Reviews']

# Print or further process results DataFrame
print(results)

import matplotlib.patches as mpatches

def plot_reviews(df):
    fig, ax = plt.subplots(figsize=(10, 6))

    # Define a colormap
    cmap = plt.cm.get_cmap('viridis')
    norm = plt.Normalize(df['Percent Positive Reviews'].min(), df['Percent Positive Reviews'].max())
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])

    # Plot bars with gradient color
    for i, (title, pos) in enumerate(zip(df['Movie Title'], df['Percent Positive Reviews'])):
        color = cmap(norm(pos))
        ax.barh(title, pos, color=color, edgecolor='black')
        ax.text(pos - 5, i, f'üëç {pos:.2f}%', va='center', ha='right', color='white', fontsize=12, fontweight='bold')

    # Add color bar legend
    cbar = plt.colorbar(sm, ax=ax)
    cbar.set_label('Percentage of Positive Reviews', rotation=270, labelpad=15)

    # Adjusting plot aesthetics
    ax.set_xlabel('Percentage of Positive Reviews')
    ax.set_title('Positive Reviews for Movies')
    ax.xaxis.set_ticks(np.arange(0, 101, 10))  # Set x-axis ticks from 0 to 100

    # Add a patch for explanation
    positive_patch = mpatches.Patch(color='lightgreen', label='Positive Reviews')
    plt.legend(handles=[positive_patch])

    plt.show()

# Example usage
plot_reviews(results)